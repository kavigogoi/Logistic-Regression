{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "      *** ASSIGNMENT ON LOGISTIC REGRESSION ***\n",
        "      *** THEORETICAL QUESTIONS AND ANSWERS ***"
      ],
      "metadata": {
        "id": "uI6KVA5FlDof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Ans 1. Logistic Regression is a statistical model used for binary classification tasks, predicting the probability that an instance belongs to a particular class. It uses a sigmoid function to map any real value to a probability between 0 and 1.\n",
        "\n",
        "Here's how it differs from Linear Regression:\n",
        "\n",
        "Target Variable: Linear Regression predicts a continuous outcome, while Logistic Regression predicts a categorical outcome (usually binary, but can be extended to multiclass).\n",
        "Output: Linear Regression directly outputs a continuous value. Logistic Regression outputs a probability, which is then typically converted into a class prediction based on a threshold (e.g., if probability > 0.5, predict class 1).\n",
        "Underlying Function: Linear Regression uses a linear function (y = mx + b) to model the relationship between features and the target. Logistic Regression uses the sigmoid function applied to a linear combination of features to model the probability of the target class.\n",
        "Loss Function: Linear Regression commonly uses Mean Squared Error (MSE). Logistic Regression uses a loss function like Cross-Entropy or Log Loss, which is suitable for probability estimation.\n"
      ],
      "metadata": {
        "id": "tpkn97dAlQc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "Ans 2. The mathematical equation of Logistic Regression relates the probability of an instance belonging to a particular class to a linear combination of its features.\n",
        "\n",
        "There are a couple of common ways to express this equation:\n",
        "\n",
        "In terms of Probability:\n",
        "\n",
        "$$ P(y=1 | \\mathbf{x}) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k)}} $$\n",
        "\n",
        "Here:\n",
        "\n",
        "$P(y=1 | \\mathbf{x})$ is the probability that the target variable $y$ is 1 (the positive class) given the feature vector $\\mathbf{x}$.\n",
        "$e$ is the base of the natural logarithm.\n",
        "$\\beta_0$ is the intercept (bias).\n",
        "$\\beta_1, \\beta_2, ..., \\beta_k$ are the coefficients (weights) for each feature $x_1, x_2, ..., x_k$.\n",
        "The expression $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k$ is the linear combination of the features, often denoted as $z$.\n",
        "This equation uses the sigmoid function (also known as the logistic function) to map the linear combination of features to a probability between 0 and 1.\n",
        "\n",
        "In terms of Log-Odds (Logit):\n",
        "\n",
        "The equation can also be expressed in terms of the log-odds (or logit) of the probability. The odds of an event is the ratio of the probability that the event will occur to the probability that it will not occur: Odds $= \\frac{P(y=1 | \\mathbf{x})}{P(y=0 | \\mathbf{x})} = \\frac{P(y=1 | \\mathbf{x})}{1 - P(y=1 | \\mathbf{x})}$.\n",
        "\n",
        "The log-odds (logit) is the natural logarithm of the odds:\n",
        "\n",
        "$$ \\text{logit}(P(y=1 | \\mathbf{x})) = \\log\\left(\\frac{P(y=1 | \\mathbf{x})}{1 - P(y=1 | \\mathbf{x})}\\right) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k $$\n",
        "\n",
        "This form shows that the log-odds of the probability is a linear combination of the features. This is why it's called \"Logistic Regression\" – it's essentially performing a linear regression on the log-odds of the target variable."
      ],
      "metadata": {
        "id": "L00xC_Enz4ZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "Ans 3. We use the Sigmoid function in Logistic Regression for the following reasons:\n",
        "\n",
        "Probability Output: The sigmoid function maps any real-valued input to an output between 0 and 1. This is crucial because the goal of logistic regression is to predict the probability of an instance belonging to a particular class. The output of the sigmoid function can be directly interpreted as this probability.\n",
        "\n",
        "Non-linearity: While the core of logistic regression involves a linear combination of features ($\\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k$$\\beta_0 + \\beta_1 x_1 + ... + \\beta_k x_k$), the sigmoid function introduces non-linearity. This allows the model to capture non-linear relationships between the features and the probability of the outcome.\n",
        "\n",
        "Smooth Gradient: The sigmoid function has a smooth, differentiable curve. This is important for the optimization algorithms (like gradient descent) used to train the logistic regression model. The smooth gradient allows the optimization process to effectively find the best model parameters (coefficients).\n",
        "\n",
        "Interpretation: The sigmoid function's S-shape is intuitive for modeling probabilities. As the linear combination of features increases, the probability approaches 1, and as it decreases, the probability approaches 0. There is a clear threshold (typically 0.5) where the model transitions from predicting one class to the other.\n",
        "\n",
        "In essence, the sigmoid function acts as a link function that transforms the linear output of the model into a probability, making Logistic Regression suitable for binary classification tasks."
      ],
      "metadata": {
        "id": "6HjghhQe0YmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 4. What is the cost function of Logistic Regression?\n",
        "\n",
        "Ans 4. In logistic regression, the cost function, often called the log loss or cross-entropy loss, measures how well the model's predictions match the actual outcomes. It's calculated by averaging the loss over all training examples.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "Loss Function (Individual Data Point):\n",
        "For a single data point, the loss function calculates the error between the model's predicted probability and the actual outcome (0 or 1).\n",
        "\n",
        "Cost Function (Overall Model Performance):\n",
        "The cost function takes the average of the loss function across all the training examples in your dataset. This provides a single value that represents the overall error of your model.\n",
        "\n",
        "Mathematical Representation:\n",
        "The cost function (J) for logistic regression is typically represented as:\n",
        "J(w, b) = (1/m) * Σ ( -y(i) * log(hθ(x(i))) - (1 - y(i)) * log(1 - hθ(x(i))) )\n",
        "\n",
        "Where:\n",
        "\n",
        "m = the number of training examples\n",
        "y(i) = the actual outcome (0 or 1) for the i-th training example\n",
        "hθ(x(i)) = the model's predicted probability (between 0 and 1) for the i-th training example, given by the sigmoid function\n",
        "w = the model's parameters (weights)\n",
        "b = the model's bias\n",
        "\n",
        "Why Log Loss?\n",
        "\n",
        "Non-convex:\n",
        "The log loss function is a suitable choice because it's convex, which means it has a single minimum. This is important for optimization algorithms like gradient descent, which can get stuck in local minima with non-convex functions.\n",
        "\n",
        "Good for Classification:\n",
        "The log loss function is well-suited for classification problems, as it provides a probability estimate for each class, allowing for more accurate predictions.\n",
        "\n",
        "Maximizes Likelihood:\n",
        "Minimizing the log loss is equivalent to maximizing the likelihood of observing the data given the model parameters, which is a common goal in statistical modeling.\n",
        "\n",
        "In Summary:\n",
        "The cost function in logistic regression is the average log loss across the training dataset. It's used to evaluate the model's performance and guide the optimization process (e.g., using gradient descent) to find the best parameters that minimize the cost function.\n"
      ],
      "metadata": {
        "id": "skQSC8540ia8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Ans 5. Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function. This penalty discourages the model from assigning excessively large weights (coefficients) to the features.\n",
        "\n",
        "Why is it needed?\n",
        "\n",
        "Logistic Regression models can sometimes overfit the training data, especially when dealing with a large number of features or complex relationships. Overfitting occurs when the model learns the training data too well, including the noise and random fluctuations, which leads to poor performance on unseen data.\n",
        "\n",
        "Regularization helps to address overfitting by:\n",
        "\n",
        "Shrinking Coefficients: The penalty term in the cost function encourages the model's coefficients to be smaller. This makes the model simpler and less sensitive to small variations in the training data.\n",
        "Reducing Model Complexity: By limiting the size of the coefficients, regularization effectively reduces the complexity of the model, making it less prone to overfitting.\n",
        "Improving Generalization: A regularized model is less likely to be overly tailored to the training data, which means it will generalize better to new, unseen data.\n",
        "Types of Regularization in Logistic Regression:\n",
        "\n",
        "There are two common types of regularization used in Logistic Regression:\n",
        "\n",
        "L1 Regularization (Lasso): This adds a penalty proportional to the absolute value of the coefficients to the cost function. L1 regularization can lead to sparse models, where some coefficients are driven exactly to zero. This is useful for feature selection.\n",
        "L2 Regularization (Ridge): This adds a penalty proportional to the square of the coefficients to the cost function. L2 regularization shrinks the coefficients towards zero but does not force them to be exactly zero. It generally provides better performance when all features are relevant.\n",
        "The strength of the regularization is controlled by a hyperparameter (often denoted as C in scikit-learn, which is the inverse of the regularization strength, or lambda). A smaller C (larger lambda) means stronger regularization."
      ],
      "metadata": {
        "id": "jsU_kYlM1E22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 6. Explain the difference between Lasso, Ridge, and Elastic Net regression?\n",
        "\n",
        "Ans 6. Lasso, Ridge, and Elastic Net are all types of regularized linear models, commonly used in regression tasks, but the concepts also apply to Logistic Regression as a way to regularize the coefficients. They differ in the type of penalty they add to the cost function to prevent overfitting.\n",
        "\n",
        "Here's a breakdown of the differences:\n",
        "\n",
        "Ridge Regression (L2 Regularization):\n",
        "\n",
        "Penalty: Adds a penalty proportional to the sum of the squared values of the coefficients ($\\sum_{j=1}^{k} \\beta_j^2$).\n",
        "Effect: Shrinks the coefficients towards zero but does not force them to be exactly zero. All features are generally retained in the model, but their impact is reduced.\n",
        "Use Case: Effective when you have many features that are all potentially relevant, and you want to reduce their overall impact to prevent overfitting. It can help with multicollinearity (high correlation between features).\n",
        "Lasso Regression (L1 Regularization):\n",
        "\n",
        "Penalty: Adds a penalty proportional to the sum of the absolute values of the coefficients ($\\sum_{j=1}^{k} |\\beta_j|$).\n",
        "Effect: Shrinks the coefficients towards zero and can force some coefficients to be exactly zero. This leads to a sparse model, effectively performing feature selection by eliminating the impact of less important features.\n",
        "Use Case: Useful when you suspect that only a subset of your features are truly relevant, and you want to automatically select the most important ones. It can be beneficial for high-dimensional datasets.\n",
        "Elastic Net Regression:\n",
        "\n",
        "Penalty: Adds a penalty that is a combination of both L1 and L2 penalties. It is controlled by two hyperparameters: one for the overall strength of the regularization (like in Ridge and Lasso) and one to balance the mix between L1 and L2.\n",
        "Effect: Combines the properties of both Ridge and Lasso. It can shrink coefficients (like Ridge) and also perform feature selection by setting some coefficients to zero (like Lasso). It is particularly useful when you have groups of highly correlated features. Lasso might arbitrarily pick one from the group, while Elastic Net tends to select all features within the group.\n",
        "Use Case: A good option when you have many features and some of them are highly correlated. It can provide a better balance between shrinking coefficients and feature selection than using L1 or L2 alone.\n",
        "In summary:\n",
        "\n",
        "Ridge: Shrinks coefficients, keeps all features, good for multicollinearity.\n",
        "Lasso: Shrinks coefficients, can set some to zero (feature selection), good for sparse models.\n",
        "Elastic Net: Combination of Ridge and Lasso, good for correlated features, can do both shrinkage and feature selection.\n",
        "The choice between them often depends on the characteristics of your data and your modeling goals (e.g., prioritizing feature selection vs. simply reducing coefficient magnitudes)."
      ],
      "metadata": {
        "id": "ndyD7Tau1qih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Ans 7. We should consider using Elastic Net instead of Lasso or Ridge in the following situations:\n",
        "\n",
        "When you have many features and some of them are highly correlated: This is one of the key scenarios where Elastic Net excels. Lasso tends to arbitrarily pick one feature from a group of highly correlated features and ignore the others. Ridge shrinks all correlated features equally. Elastic Net, by combining L1 and L2 penalties, tends to select all features within a correlated group. This can lead to better model performance and interpretability when dealing with multicollinearity.\n",
        "\n",
        "When you suspect that only a subset of features are truly relevant, but you also have groups of correlated features: If you want the feature selection capability of Lasso (driving some coefficients to zero) but also want to handle correlated features more gracefully than Lasso does, Elastic Net is a good compromise.\n",
        "\n",
        "When you are unsure whether L1 or L2 regularization is more appropriate: Elastic Net allows you to tune the balance between L1 and L2 penalties using the l1_ratio hyperparameter. If l1_ratio is 1, it's equivalent to Lasso; if it's 0, it's equivalent to Ridge. This makes Elastic Net a more flexible option that can adapt to different data characteristics.\n",
        "\n",
        "When you want a more stable solution than Lasso when features are highly correlated: Due to its tendency to select only one feature from a correlated group, Lasso's performance can be unstable if you have highly correlated features and rerun the analysis on slightly different data or splits. Elastic Net's L2 component helps to stabilize the solution in such cases.\n",
        "\n",
        "In summary, Elastic Net is a powerful regularized regression method that combines the benefits of Lasso and Ridge. It's particularly useful when dealing with datasets that have both many features and significant correlations among them, providing a balance between feature selection and coefficient shrinkage."
      ],
      "metadata": {
        "id": "aZsnHEeS157O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 8. What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
        "\n",
        "Ans 8. The regularization parameter, often denoted as $\\lambda$$\\lambda$ (lambda) or represented by C in libraries like scikit-learn (where C is the inverse of $\\lambda$$\\lambda$), controls the strength of the regularization applied in Logistic Regression. It has a significant impact on the model's complexity and its tendency to overfit or underfit the data.\n",
        "\n",
        "Here's the impact of the regularization parameter:\n",
        "\n",
        "Controls the Penalty Strength:\n",
        "\n",
        "Larger $\\lambda$ (or smaller C): A larger $\\lambda$ means a stronger penalty is added to the cost function. This forces the model to have smaller coefficients. In the case of L1 regularization, it can even drive some coefficients exactly to zero, performing feature selection.\n",
        "Smaller $\\lambda$ (or larger C): A smaller $\\lambda$ means a weaker penalty. The model is less constrained, and coefficients can become larger. With a very small $\\lambda$ (or very large C), the model approaches standard Logistic Regression without regularization.\n",
        "Impact on Coefficient Magnitudes:\n",
        "\n",
        "As $\\lambda$ increases, the magnitudes of the coefficients decrease. This is the core mechanism of regularization – it shrinks the coefficients towards zero.\n",
        "Impact on Model Complexity:\n",
        "\n",
        "Larger $\\lambda$ leads to a simpler model with smaller coefficients. This reduces the model's capacity to fit the noise in the training data, thereby reducing the risk of overfitting.\n",
        "Smaller $\\lambda$ leads to a more complex model with larger coefficients. This allows the model to fit the training data more closely, but it increases the risk of overfitting, especially on noisy or limited data.\n",
        "Impact on Bias-Variance Trade-off:\n",
        "\n",
        "Regularization is a technique used to manage the bias-variance trade-off.\n",
        "Larger $\\lambda$ increases the bias of the model (it might not fit the training data as well) but decreases the variance (it becomes less sensitive to the specific training set and generalizes better to unseen data). This helps to reduce overfitting.\n",
        "Smaller $\\lambda$ decreases the bias (fits the training data more closely) but increases the variance (becomes more sensitive to the training set). This increases the risk of overfitting.\n",
        "Impact on Feature Selection (L1 Regularization):\n",
        "\n",
        "With L1 regularization, increasing $\\lambda$ will cause more coefficients to be driven exactly to zero. This effectively performs feature selection, as features with zero coefficients are excluded from the model.\n",
        "Choosing the right value for the regularization parameter ($\\lambda$$\\lambda$ or C) is crucial and is typically done through hyperparameter tuning techniques like cross-validation. The goal is to find a value that provides a good balance between fitting the training data and generalizing well to unseen data."
      ],
      "metadata": {
        "id": "e1rTiJVd2UOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "Ans 9. Logistic regression relies on several key assumptions for accurate and reliable results. These include the independence of observations, linearity of the log-odds, absence of multicollinearity, and a sufficiently large sample size. Additionally, the outcome variable must be categorical, and outliers should be addressed.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "Independence of Observations:\n",
        "Each data point should be independent of the others. This means the outcome for one observation should not be influenced by the outcome of any other observation.\n",
        "\n",
        "Linearity of the Logit (Log-Odds):\n",
        "While logistic regression doesn't require a linear relationship between the independent variables and the dependent variable, it does assume a linear relationship between the independent variables and the log-odds of the dependent variable.\n",
        "\n",
        "Absence of Multicollinearity:\n",
        "High correlation between independent variables can make it difficult to determine the individual impact of each variable on the outcome. It's crucial to minimize multicollinearity in the dataset.\n",
        "\n",
        "Sufficiently Large Sample Size:\n",
        "A large enough sample size is needed to ensure stable and reliable coefficient estimates. A general guideline is to have at least 50 observations per predictor variable, according to Data-Mania.\n",
        "\n",
        "Binary Outcome Variable (for Binary Logistic Regression):\n",
        "If using binary logistic regression, the dependent variable should have only two possible outcomes.\n",
        "\n",
        "Absence of Strongly Influential Outliers:\n",
        "Outliers can significantly impact the model's results. Identifying and addressing outliers is important for robust analysis"
      ],
      "metadata": {
        "id": "UcS22TRD2ih8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Ans 10. Several alternative methods can be used for classification tasks instead of logistic regression. These include Support Vector Machines (SVM), Decision Trees, Random Forests, and Neural Networks. Additionally, ensemble methods like boosting and bagging can also be used to improve classification performance.\n",
        "\n",
        "Here's a more detailed look at some of these alternatives:\n",
        "\n",
        "1. Support Vector Machines (SVM):\n",
        "SVMs are powerful classifiers that work by finding the optimal hyperplane to separate data points into different classes.\n",
        "They are particularly effective when dealing with high-dimensional data or when the classes are well-separated.\n",
        "SVMs can be used for both linear and non-linear classification problems by using different kernel functions.\n",
        "A variation of SVM, called Support Vector Regression (SVR), is used for regression tasks.\n",
        "\n",
        "2. Decision Trees:\n",
        "Decision trees are tree-like structures that recursively partition the data space based on feature values.\n",
        "They are relatively easy to interpret and can handle both numerical and categorical data.\n",
        "Decision trees can be prone to overfitting, but this can be mitigated by techniques like pruning or using ensemble methods.\n",
        "\n",
        "3. Random Forests:\n",
        "Random forests are ensemble methods that combine multiple decision trees to improve classification accuracy and reduce overfitting.\n",
        "They are known for their robustness and ability to handle complex relationships between features.\n",
        "Random forests are widely used in various classification tasks and often achieve high accuracy.\n",
        "\n",
        "4. Neural Networks:\n",
        "Neural networks, especially deep learning models, are capable of learning complex patterns in data.\n",
        "They can be used for both binary and multi-class classification problems.\n",
        "Neural networks require large amounts of data and computational resources for training.\n",
        "\n",
        "5. Ensemble Methods:\n",
        "Ensemble methods combine multiple individual models to improve overall performance.\n",
        "Boosting and bagging are popular ensemble techniques that can be used with various base classifiers, including decision trees.\n",
        "Ensemble methods often lead to more accurate and robust classification results.\n",
        "\n",
        "6. Other Alternatives:\n",
        "\n",
        "k-Nearest Neighbors (k-NN):\n",
        "A simple non-parametric method that classifies data points based on the majority class among their k nearest neighbors.\n",
        "\n",
        "Naive Bayes:\n",
        "A probabilistic classifier based on Bayes' theorem, assuming independence between features.\n",
        "\n",
        "Linear Discriminant Analysis (LDA):\n",
        "A dimensionality reduction technique that can also be used for classification.\n",
        "\n",
        "Probit Regression:\n",
        "Similar to logistic regression, but uses the probit function instead of the sigmoid function.\n",
        "\n",
        "Log-binomial and Poisson Regression:\n",
        "Alternative models for analyzing binary outcomes, particularly in cross-sectional studies.\n"
      ],
      "metadata": {
        "id": "grZf1Ul83Ch7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 11. What are Classification Evaluation Metrics?\n",
        "\n",
        "Ans 11. Common classification evaluation metrics include accuracy, precision, recall, F1-score, AUC-ROC, and the confusion matrix. These metrics help assess how well a classification model distinguishes between different classes.\n",
        "\n",
        "Here's a breakdown of some key metrics:\n",
        "\n",
        "Accuracy:\n",
        "The proportion of correctly classified instances out of the total instances. It's a good general measure, but can be misleading with imbalanced datasets (where one class has significantly more instances than others).\n",
        "\n",
        "Precision:\n",
        "Out of all instances predicted as positive, what proportion is actually positive? High precision indicates fewer false positives.\n",
        "\n",
        "Recall (also known as Sensitivity):\n",
        "Out of all actual positive instances, what proportion did the model correctly identify? High recall indicates fewer false negatives.\n",
        "\n",
        "F1-Score:\n",
        "The harmonic mean of precision and recall. It provides a balanced measure that considers both false positives and false negatives, especially useful when there's an imbalance between classes.\n",
        "\n",
        "AUC-ROC:\n",
        "The area under the Receiver Operating Characteristic curve. It measures the model's ability to distinguish between classes at various threshold settings, particularly useful for binary classification problems.\n",
        "\n",
        "Confusion Matrix:\n",
        "A table that summarizes the performance of a classification model by showing the counts of true positives, true negatives, false positives, and false negatives.\n"
      ],
      "metadata": {
        "id": "xGApb0Rf3cUU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "Ans 12. Class imbalance, where one class has significantly more instances than others, can negatively impact logistic regression by causing the model to be biased towards the majority class, leading to poor performance on the minority class. This bias arises because logistic regression aims to optimize overall accuracy, which is easily achieved by favoring the majority class.\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "Bias towards the majority class:\n",
        "Logistic regression, in its standard form, aims to maximize overall accuracy, which can be misleading in imbalanced datasets. Because the majority class dominates the training data, the model can achieve high accuracy simply by predicting the majority class for most, if not all, instances.\n",
        "\n",
        "Poor minority class prediction:\n",
        "This bias results in the model misclassifying a large portion of the minority class, as it doesn't learn enough about its characteristics. Rare events or minority classes might be completely ignored or misclassified, leading to significant errors in practical applications.\n",
        "\n",
        "Impact on model evaluation:\n",
        "Traditional metrics like accuracy can be misleading in imbalanced scenarios. A model with high accuracy might still be poor at predicting the minority class, making it crucial to use metrics like precision, recall, F1-score, and AUC-ROC, which are more sensitive to class imbalance.\n",
        "\n",
        "Example:\n",
        "In fraud detection, where fraudulent transactions are rare, a logistic regression model trained on an imbalanced dataset might have high accuracy by correctly classifying the majority of legitimate transactions, but it would likely misclassify a large proportion of fraudulent transactions.\n",
        "\n",
        "Addressing class imbalance:\n",
        "Several techniques can be employed to mitigate the effects of class imbalance, such as resampling methods (oversampling the minority class or undersampling the majority class), using different evaluation metrics, or adjusting the model's cost function to penalize misclassification of the minority class more heavily."
      ],
      "metadata": {
        "id": "PXArBNvc3re_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Ans 13. Hyperparameter tuning in logistic regression is the process of finding the optimal values for the settings (hyperparameters) that control the learning process before training the model. These hyperparameters influence the model's performance, complexity, and learning speed. By tuning these, you can improve the model's ability to generalize and make accurate predictions on new, unseen data.\n",
        "\n",
        " Essential Hyperparameter Tuning Techniques to Know\n",
        "\n",
        "Here's a more detailed explanation:\n",
        "\n",
        "What are Hyperparameters?\n",
        "Hyperparameters are not learned by the model during training, unlike the model's internal parameters (coefficients). Instead, they are set before training begins and define how the learning algorithm behaves.\n",
        "\n",
        "Why Tune Hyperparameters?\n",
        "Improved Model Performance:\n",
        "Tuning can lead to better accuracy, precision, recall, and other relevant metrics for your specific task.\n",
        "\n",
        "Reduced Overfitting/Underfitting:\n",
        "Proper hyperparameter tuning can help prevent the model from being too complex (overfitting) or too simple (underfitting).\n",
        "\n",
        "Enhanced Generalization:\n",
        "A well-tuned model will generalize better to new, unseen data, meaning it will make more accurate predictions on data it hasn't been trained on.\n",
        "\n",
        "Common Hyperparameters in Logistic Regression:\n",
        "\n",
        "C (Regularization Strength):\n",
        "Controls the trade-off between achieving a low error on the training data and keeping the model simple to prevent overfitting. Higher values of C (or lower regularization) mean the model will fit the training data more closely, potentially leading to overfitting.\n",
        "\n",
        "solver:\n",
        "Determines the algorithm used to optimize the model's parameters. Options include 'liblinear', 'lbfgs', 'newton-cg', 'sag', and 'saga'. Each solver has different strengths and weaknesses, and the best choice depends on the dataset.\n",
        "\n",
        "max_iter:\n",
        "Specifies the maximum number of iterations the solver is allowed to run. If the solver doesn't converge within this limit, it will stop and return the best result found so far.\n",
        "\n",
        "penalty:\n",
        "Specifies the type of regularization to use (e.g., L1, L2, or None).\n",
        "\n",
        "tol:\n",
        "Sets the tolerance for the optimization process, determining when the algorithm stops.\n",
        "\n",
        "Hyperparameter Tuning Techniques:\n",
        "\n",
        "Grid Search:\n",
        "Systematically tries all possible combinations of hyperparameters within a specified range.\n",
        "\n",
        "Random Search:\n",
        "Randomly samples hyperparameter combinations.\n",
        "\n",
        "Bayesian Optimization:\n",
        "Uses a probabilistic model to guide the search for optimal hyperparameters, learning from previous evaluations.\n",
        "\n",
        "Cross-validation:\n",
        "A technique used to evaluate the model's performance with different hyperparameter settings, helping to prevent overfitting.\n",
        "\n",
        "By carefully choosing the appropriate hyperparameters, you can significantly improve the performance and reliability of your logistic regression model."
      ],
      "metadata": {
        "id": "huv7m5Jj4Aav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "Ans 14. In Logistic Regression, different solvers are optimization algorithms used to find the best model parameters. The choice of solver depends on the size and characteristics of your dataset, as well as the specific requirements of your problem.\n",
        "Here's a breakdown of some common solvers and when to use them:\n",
        "\n",
        "liblinear:\n",
        "This solver is efficient for small datasets, especially when the data is sparse (many features are zero). It's also a good choice for binary classification problems. It can handle L1 regularization (sparse penalties).\n",
        "\n",
        "lbfgs (Limited-memory Broyden–Fletcher–Goldfarb–Schanz):\n",
        "This solver is generally recommended for medium-sized datasets. It's considered more robust than liblinear and can work well with denser data. It's a good default choice.\n",
        "\n",
        "sag (Stochastic Average Gradient):\n",
        "This solver is designed for large datasets. It updates the parameters by considering only a random subset of the data at each iteration, making it computationally efficient. It's suitable for both dense and sparse data.\n",
        "\n",
        "saga (Stochastic Accelerated Gradient):\n",
        "Similar to SAG, but it includes an additional correction term that improves convergence. This makes it particularly effective for large datasets with high dimensionality or when using elastic net regularization.\n",
        "\n",
        "newton-cg (Newton's Method with Conjugate Gradients):\n",
        "This solver is fast for datasets where the number of samples is significantly larger than the number of features and features are one-hot encoded. However, it has high memory usage due to the computation of the full Hessian matrix.\n",
        "\n",
        "Which solver to use:\n",
        "For small datasets: Use liblinear.\n",
        "For medium-sized datasets: Use lbfgs.\n",
        "For large datasets: Use sag or saga.\n",
        "For sparse data: Use liblinear or saga.\n",
        "For multiclass classification: All solvers except liblinear can handle this, except for liblinear which can only handle binary classification.\n",
        "\n",
        "For more nuanced advice on solver selection, consider the specific characteristics of your dataset (size, sparsity, number of features, etc.) and the requirements of your application (accuracy, computational time, memory constraints). Experimenting with different solvers and comparing their performance on your validation data can help you find the optimal solution."
      ],
      "metadata": {
        "id": "iUs8wNrn4i13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "Ans 15. Logistic Regression, primarily designed for binary classification, can be extended to handle multiclass problems using techniques like One-vs-Rest (OvR), One-vs-One (OvO), or Multinomial Logistic Regression (also known as Softmax Regression). These methods adapt the binary classifier to predict probabilities for multiple classes.\n",
        "\n",
        "1. One-vs-Rest (OvR) or One-vs-All (OvA):\n",
        "This strategy trains a separate binary logistic regression model for each class, where one class is treated as positive and all other classes are combined as the negative class.\n",
        "For example, if you have three classes (A, B, C), you would train three models: A vs. (B+C), B vs. (A+C), and C vs. (A+B).\n",
        "During prediction, the model that outputs the highest probability for a given input is selected as the predicted class.\n",
        "This approach is simple to implement and computationally efficient.\n",
        "\n",
        "2. One-vs-One (OvO):\n",
        "In OvO, a binary logistic regression model is trained for every pair of classes.\n",
        "With N classes, you would have N*(N-1)/2 models.\n",
        "For example, with classes A, B, and C, you would train models: A vs B, A vs C, and B vs C.\n",
        "During prediction, each model votes for one of the two classes it's comparing, and the class with the most votes wins.\n",
        "OvO can be more accurate than OvR but is computationally more expensive.\n",
        "\n",
        "3. Multinomial Logistic Regression (Softmax Regression):\n",
        "This is a direct extension of logistic regression that models the probabilities of all classes simultaneously.\n",
        "It uses the softmax function to output a probability distribution over all classes.\n",
        "\n",
        "The class with the highest probability is chosen as the prediction.\n",
        "It's more efficient than OvR or OvO for a large number of classes but can be computationally intensive.\n",
        "\n",
        "Multinomial Logistic Regression is often preferred when dealing with categorical dependent variables with more than two categories, according to Laerd Statistics.\n"
      ],
      "metadata": {
        "id": "FMpIfJib49Ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "Ans 16. Here are the advantages and disadvantages of Logistic Regression:\n",
        "\n",
        "Advantages:\n",
        "\n",
        "Simplicity and Interpretability: Logistic Regression is relatively simple to understand and interpret, especially compared to more complex models like neural networks. The coefficients directly indicate the impact of each feature on the log-odds of the outcome.\n",
        "\n",
        "Efficiency: Training Logistic Regression models is generally fast and computationally efficient, making it suitable for large datasets.\n",
        "\n",
        "Good Baseline Model: It often serves as a good baseline model for classification tasks. If Logistic Regression performs well, it might be sufficient, or it can be used as a benchmark to compare against more complex algorithms.\n",
        "\n",
        "Outputs Probabilities: Logistic Regression outputs probabilities of belonging to a class, which can be useful for understanding the confidence of the prediction and for setting different classification thresholds.\n",
        "\n",
        "Less Prone to Overfitting with Regularization: With the help of regularization techniques (L1, L2, Elastic Net), Logistic Regression can effectively prevent overfitting, especially when dealing with many features.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "Assumes Linearity of Log-Odds: While it doesn't assume a linear relationship between features and the outcome itself, it assumes a linear relationship between the features and the log-odds of the outcome. If this assumption is violated, the model's performance may be poor.\n",
        "\n",
        "Difficulty with Complex Relationships: Logistic Regression struggles to capture complex, non-linear relationships between features and the target variable.\n",
        "Sensitive to Outliers: Like Linear Regression, Logistic Regression can be sensitive to outliers in the data, which can disproportionately influence the model coefficients.\n",
        "\n",
        "Assumes Independence of Observations: It assumes that the observations are independent. If there are dependencies between data points, the model might not perform optimally.\n",
        "\n",
        "Can be Affected by Multicollinearity: High correlation between features can lead to unstable coefficient estimates and make it difficult to interpret the individual impact of each feature.\n",
        "Regularization can help mitigate this.\n",
        "\n",
        "Limited to Binary Outcomes (in its basic form): The fundamental Logistic Regression model is designed for binary classification. While extensions like OvR, OvO, and Multinomial Logistic Regression exist for multiclass problems, they can add complexity."
      ],
      "metadata": {
        "id": "W_FS8RZS5af5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 17. What are some use cases of Logistic Regression?\n",
        "\n",
        "Ans 17. Logistic regression is used when the outcome variable is categorical, specifically when it has two or more discrete outcomes. It's a classification algorithm that estimates the probability of an instance belonging to a particular category.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "When to use Logistic Regression:\n",
        "\n",
        "Binary Classification:\n",
        "The most common application is for binary classification problems, where there are only two possible outcomes (e.g., yes/no, true/false, spam/not spam).\n",
        "\n",
        "Multi-class Classification:\n",
        "Logistic regression can also be extended to handle cases with more than two categories (multinomial logistic regression), though other algorithms like softmax regression are often preferred in these scenarios.\n",
        "\n",
        "Predicting Probabilities:\n",
        "Instead of just predicting a category, logistic regression provides the probability of an instance belonging to each category, which can be useful in many applications.\n",
        "\n",
        "Modeling Nonlinear Relationships:\n",
        "Logistic regression can handle nonlinear relationships between variables, making it suitable for situations where a linear model might not provide accurate predictions.\n",
        "\n",
        "Examples of Logistic Regression Use Cases:\n",
        "Medical Diagnosis: Predicting whether a patient has a disease based on various health indicators.\n",
        "\n",
        "Fraud Detection: Identifying fraudulent transactions in banking or insurance.\n",
        "\n",
        "Marketing: Predicting customer churn or the likelihood of a customer purchasing a product.\n",
        "\n",
        "Spam Filtering: Classifying emails as spam or not spam.\n",
        "\n",
        "Risk Assessment: Evaluating the risk of loan defaults or other potential losses.\n",
        "\n",
        "Natural Language Processing: Analyzing text data to determine sentiment or categorize documents.\n",
        "\n",
        "Key Differences from Linear Regression:\n",
        "\n",
        "Dependent Variable:\n",
        "Linear regression is used for continuous dependent variables, while logistic regression is used for categorical dependent variables.\n",
        "\n",
        "Output:\n",
        "Linear regression outputs a continuous value, while logistic regression outputs probabilities or class labels.\n",
        "\n",
        "In essence, logistic regression is a powerful tool for classification problems where the outcome is discrete, and it can be applied in a wide range of fields.\n"
      ],
      "metadata": {
        "id": "FpKJ2bwc58Xx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Ans 18. The key distinction lies in the type of classification problem they are designed for:\n",
        "\n",
        "Logistic Regression: This is fundamentally a binary classification algorithm. It's used when you have two possible outcomes or classes to predict (e.g., spam or not spam, sick or not sick). It uses the sigmoid function to output a probability between 0 and 1, representing the likelihood of an instance belonging to the positive class.\n",
        "\n",
        "Softmax Regression (Multinomial Logistic Regression): This is an extension of Logistic Regression designed for multiclass classification. It's used when you have more than two possible outcomes or classes to predict (e.g., classifying images of different animal species: cat, dog, bird). Instead of the sigmoid function, it uses the softmax function to output a probability distribution over all classes. This means the output is a set of probabilities, one for each class, that sum up to 1. The class with the highest probability is the predicted class.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "If you have two classes, use Logistic Regression.\n",
        "If you have more than two classes, use Softmax Regression."
      ],
      "metadata": {
        "id": "5l7C5uiO6Xum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "Ans 19. The choice between One-vs-Rest (OvR) and Softmax for multiclass classification depends on the characteristics of your data and the desired properties of your model. OvR is a simpler, often faster, approach, especially beneficial when classes are relatively independent. Softmax, on the other hand, is generally more accurate, particularly when class relationships are important, but it can be more computationally expensive and require more careful tuning.\n",
        "\n",
        "Here's a more detailed breakdown:\n",
        "\n",
        "One-vs-Rest (OvR):\n",
        "\n",
        "How it works:\n",
        "Trains a separate binary classifier for each class, treating it as positive and all other classes as negative.\n",
        "\n",
        "Strengths:\n",
        "Simpler to implement and understand: It breaks down a complex problem into simpler binary classification tasks.\n",
        "\n",
        "Faster training: Can be faster, especially with many classes, as each binary classifier can be trained in parallel.\n",
        "\n",
        "Flexibility: Allows for using different algorithms for different classes if needed.\n",
        "\n",
        "Weaknesses:\n",
        "Can be less accurate: Doesn't explicitly model the relationships between classes.\n",
        "\n",
        "Potential for class imbalance: If one class is significantly larger than others, it can affect the performance of that classifier."
      ],
      "metadata": {
        "id": "j5-pE1Gr7MIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ques 20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Ans 20. In logistic regression, coefficients represent the change in the log-odds of the outcome for a one-unit change in the predictor variable, holding all other variables constant. These coefficients are not directly interpretable in terms of probability but can be converted to odds ratios for a more intuitive understanding of how predictors influence the likelihood of an event.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "Log-odds:\n",
        "Logistic regression models the relationship between predictors and the outcome using the log-odds (logit) of the event occurring.\n",
        "\n",
        "Coefficients:\n",
        "The coefficients in the logistic regression equation represent the change in the log-odds for a one-unit increase in the corresponding predictor.\n",
        "\n",
        "Odds Ratio:\n",
        "Exponentiating the coefficient (ecoefficient) gives the odds ratio. The odds ratio indicates how much the odds of the event change for a one-unit increase in the predictor, holding other variables constant.\n",
        "\n",
        "Interpretation:\n",
        "Positive coefficients/odds ratios > 1: An increase in the predictor is associated with an increased likelihood of the event.\n",
        "\n",
        "Negative coefficients/odds ratios < 1: An increase in the predictor is associated with a decreased likelihood of the event.\n",
        "Coefficient/odds ratio = 1: The predictor has no effect on the odds of the event.\n",
        "\n",
        "Example:\n",
        "Let's say you're modeling the probability of a customer churning (or not churning) based on their monthly charges. A positive coefficient for \"Monthly Charges\" in your logistic regression model means that as monthly charges increase, the log-odds of a customer churning also increase. If you exponentiate that coefficient and get an odds ratio of 1.1, it means that for every one-unit increase in monthly charges, the odds of churning increase by 10% (1.1 - 1 = 0.1, or 10%).\n",
        "\n",
        "Important Considerations:\n",
        "\n",
        "Statistical Significance:\n",
        "Always check the statistical significance of the coefficients (e.g., using p-values or z-values) to determine if the observed relationships are likely due to chance or real effects.\n",
        "\n",
        "Context:\n",
        "The interpretation of coefficients can be more complex when dealing with interaction terms or categorical variables. It's crucial to understand how these factors influence the relationships within the model."
      ],
      "metadata": {
        "id": "6ES_E0Fh7fRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "      *** PRACTICAL QUESTIONS AND ANSWERS ***"
      ],
      "metadata": {
        "id": "r5uDzjHM9YAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "# Solution 1.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression model accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6s6ZY5-lRHM",
        "outputId": "55a911d9-8c04-4bb3-b346-2ac05427f2bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "# Solution 2.\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model with L1 regularization\n",
        "# Use liblinear solver for L1 penalty\n",
        "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model_l1.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred_l1 = model_l1.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n",
        "print(f\"Logistic Regression with L1 regularization model accuracy: {accuracy_l1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSCz68E3l8D4",
        "outputId": "20aea022-1d85-4f41-f184-e8df16fcdd28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L1 regularization model accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "# Solution 3.\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model with L2 regularization\n",
        "model_l2 = LogisticRegression(penalty='l2', max_iter=200)\n",
        "model_l2.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred_l2 = model_l2.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
        "print(f\"Logistic Regression with L2 regularization model accuracy: {accuracy_l2:.2f}\")\n",
        "\n",
        "# Print the coefficients\n",
        "print(\"Model coefficients (weights) for each feature:\")\n",
        "print(model_l2.coef_)\n",
        "print(\"Model intercept (bias):\")\n",
        "print(model_l2.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8l7c8m4mKWK",
        "outputId": "f58dbdcd-5564-4d60-e8be-a3f62a956363"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with L2 regularization model accuracy: 1.00\n",
            "Model coefficients (weights) for each feature:\n",
            "[[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n",
            "Model intercept (bias):\n",
            "[  9.00884295   1.86902164 -10.87786459]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "# Solution 4.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model with Elastic Net regularization\n",
        "# Use 'saga' solver for elasticnet penalty\n",
        "# The l1_ratio parameter controls the mix of L1 and L2\n",
        "model_elasticnet = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "model_elasticnet.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred_elasticnet = model_elasticnet.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_elasticnet = accuracy_score(y_test, y_pred_elasticnet)\n",
        "print(f\"Logistic Regression with Elastic Net regularization model accuracy: {accuracy_elasticnet:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHjRB0onmdoy",
        "outputId": "c72cb22e-9902-4c11-c38f-b489692d6cdf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with Elastic Net regularization model accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
        "\n",
        "# Solution 5.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model for multiclass classification using 'ovr'\n",
        "# 'ovr' stands for One-vs-Rest (or One-vs-All)\n",
        "model_ovr = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "model_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred_ovr = model_ovr.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Logistic Regression with 'ovr' multiclass strategy accuracy: {accuracy_ovr:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYanurmimvYw",
        "outputId": "1ebcded7-9888-4b2a-b538-c157cb52acd5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with 'ovr' multiclass strategy accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "# Solution 6.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # liblinear supports both l1 and l2\n",
        "}\n",
        "\n",
        "# Create Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and model\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with Best Parameters: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHT5Imy9nV8n",
        "outputId": "e5fc579d-8f4f-4092-b190-ae9f314f337f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Accuracy with Best Parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "# Solution 7.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Create a Logistic Regression model instance\n",
        "logreg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Create a Stratified K-Fold cross-validation object\n",
        "# n_splits is the number of folds (e.g., 5 or 10 are common)\n",
        "# shuffle=True shuffles the data before splitting\n",
        "# random_state is for reproducibility\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "# cross_val_score returns an array of accuracy scores for each fold\n",
        "scores = cross_val_score(logreg, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print the accuracy scores for each fold\n",
        "print(\"Accuracy scores for each fold:\")\n",
        "print(scores)\n",
        "\n",
        "# Calculate and print the average accuracy\n",
        "average_accuracy = np.mean(scores)\n",
        "print(f\"Average accuracy across all folds: {average_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMcaCsR-nl3v",
        "outputId": "a97b15b2-cdb5-4a80-cced-163ffabcc19b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores for each fold:\n",
            "[1.         0.96666667 0.93333333 1.         0.93333333]\n",
            "Average accuracy across all folds: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "# Solution 8.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Assuming you have a CSV file named 'your_dataset.csv'\n",
        "# Make sure the CSV file is uploaded to your Colab environment or mounted from Google Drive\n",
        "# Replace 'your_dataset.csv' with the actual filename and path\n",
        "\n",
        "try:\n",
        "    # Load the dataset from a CSV file\n",
        "    # Replace 'your_dataset.csv' with the name of your CSV file\n",
        "    df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "    # Assuming your target variable is in the last column and features are in the other columns\n",
        "    # Adjust the column indexing if your dataset has a different structure\n",
        "    X = df.iloc[:, :-1]  # Features (all columns except the last one)\n",
        "    y = df.iloc[:, -1]   # Target variable (the last column)\n",
        "\n",
        "    # Split the data into training and testing sets (80% train, 20% test)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create and train the Logistic Regression model\n",
        "    model = LogisticRegression(max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict using the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate and print the accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Logistic Regression model accuracy on the test set: {accuracy:.2f}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'your_dataset.csv' not found. Please make sure the CSV file is in the correct location.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGuiSN73n1K5",
        "outputId": "5022c111-307d-4228-86ae-48d8ee31c273"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: could not convert string to float: 'Female'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "# Solution 9.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "from sklearn.datasets import make_classification\n",
        "from scipy.stats import uniform, randint\n",
        "\n",
        "# Generate a synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid for RandomizedSearchCV\n",
        "param_distributions = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'C': uniform(0.001, 10),\n",
        "    'solver': ['liblinear', 'saga', 'lbfgs', 'newton-cg', 'newton-cholesky']\n",
        "}\n",
        "\n",
        "# Create the Logistic Regression model\n",
        "logistic = LogisticRegression(random_state=42, max_iter=10000)\n",
        "\n",
        "# Create the RandomizedSearchCV object\n",
        "random_search = RandomizedSearchCV(\n",
        "    logistic,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,  # Number of random combinations to try\n",
        "    cv=5,       # Number of cross-validation folds\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    n_jobs=-1,    # Use all available cores\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Fit the RandomizedSearchCV object to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"Best parameters:\", best_params)\n",
        "print(\"Best cross-validation accuracy:\", best_score)\n",
        "\n",
        "# Evaluate on the test set\n",
        "best_model = random_search.best_estimator_\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "print(\"Test accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6s1jXzKoj9T",
        "outputId": "c21f4783-5250-414b-b6ac-bb8a1df6168a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'C': np.float64(0.1573640674119393), 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Best cross-validation accuracy: 0.8712500000000001\n",
            "Test accuracy: 0.875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "170 fits failed out of a total of 250.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "71 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'l2', 'elasticnet'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cholesky supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "9 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "5 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 63, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver newton-cg supports only 'l2' or None penalties, got elasticnet penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "# Solution 10.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Logistic Regression base estimator\n",
        "base_estimator = LogisticRegression(max_iter=200, solver='liblinear') # Using liblinear for efficiency with OvO\n",
        "\n",
        "# Create a One-vs-One multiclass classifier\n",
        "# This will train a separate Logistic Regression model for every pair of classes\n",
        "model_ovo = OneVsOneClassifier(base_estimator)\n",
        "\n",
        "# Train the OvO classifier\n",
        "model_ovo.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred_ovo = model_ovo.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "print(f\"One-vs-One Multiclass Logistic Regression accuracy: {accuracy_ovo:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_MZMmm5pOL3",
        "outputId": "8c38492a-0e61-445f-a365-3efac4f2d366"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Multiclass Logistic Regression accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "# Solution 11.\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cnf_matrix)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cnf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
        "            yticklabels=['Actual Negative', 'Actual Positive'])\n",
        "plt.xlabel('Predicted label')\n",
        "plt.ylabel('Actual label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "9VQZBahypYA-",
        "outputId": "cd660774-f952-4ae2-a562-cd1aba1e1ef3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model accuracy: 0.83\n",
            "\n",
            "Confusion Matrix:\n",
            "[[75 14]\n",
            " [20 91]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.84      0.82        89\n",
            "           1       0.87      0.82      0.84       111\n",
            "\n",
            "    accuracy                           0.83       200\n",
            "   macro avg       0.83      0.83      0.83       200\n",
            "weighted avg       0.83      0.83      0.83       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "# Solution 12.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy (optional, but good to see alongside other metrics)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression model accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Calculate and print Precision\n",
        "# Precision is the ratio TP / (TP + FP)\n",
        "# It answers: Of all the instances predicted as positive, how many were actually positive?\n",
        "precision = precision_score(y_test, y_pred)\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "\n",
        "# Calculate and print Recall (Sensitivity)\n",
        "# Recall is the ratio TP / (TP + FN)\n",
        "# It answers: Of all the actual positive instances, how many were correctly predicted as positive?\n",
        "recall = recall_score(y_test, y_pred)\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "\n",
        "# Calculate and print F1-Score\n",
        "# The F1-score is the harmonic mean of the precision and recall\n",
        "# F1 = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "# It provides a single score that balances both precision and recall.\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV_YrtKvrtvn",
        "outputId": "5e8ee811-1391-4dff-fab6-df448166ae0f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model accuracy: 0.83\n",
            "Precision: 0.87\n",
            "Recall: 0.82\n",
            "F1-Score: 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "# Solution 13.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Generate some imbalanced data\n",
        "# Let's create a dataset with a significant imbalance between two classes\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "n_features = 5\n",
        "X = np.random.rand(n_samples, n_features)\n",
        "\n",
        "# Create a target variable with a large majority class (e.g., 90% class 0)\n",
        "y = np.zeros(n_samples, dtype=int)\n",
        "minority_class_size = int(0.1 * n_samples) # 10% minority class\n",
        "minority_indices = np.random.choice(n_samples, size=minority_class_size, replace=False)\n",
        "y[minority_indices] = 1\n",
        "\n",
        "print(f\"Original dataset class distribution: Class 0: {np.sum(y == 0)}, Class 1: {np.sum(y == 1)}\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set class distribution: Class 0: {np.sum(y_train == 0)}, Class 1: {np.sum(y_train == 1)}\")\n",
        "print(f\"Testing set class distribution: Class 0: {np.sum(y_test == 0)}, Class 1: {np.sum(y_test == 1)}\")\n",
        "\n",
        "# 1. Train a Logistic Regression model without class weights\n",
        "print(\"\\nTraining Logistic Regression without class weights...\")\n",
        "model_no_weights = LogisticRegression(random_state=42)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model without weights\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "print(\"\\nClassification Report (No Class Weights):\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "print(\"Confusion Matrix (No Class Weights):\")\n",
        "print(confusion_matrix(y_test, y_pred_no_weights))\n",
        "\n",
        "# 2. Train a Logistic Regression model with class weights\n",
        "# Use 'balanced' to automatically adjust weights inversely proportional to class frequencies\n",
        "print(\"\\nTraining Logistic Regression with 'balanced' class weights...\")\n",
        "model_with_weights = LogisticRegression(random_state=42, class_weight='balanced')\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model with weights\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "print(\"\\nClassification Report (With Class Weights = 'balanced'):\")\n",
        "print(classification_report(y_test, y_pred_with_weights))\n",
        "print(\"Confusion Matrix (With Class Weights = 'balanced'):\")\n",
        "print(confusion_matrix(y_test, y_pred_with_weights))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftBE_kvfr3gS",
        "outputId": "f8035dde-eb52-4dff-9bc1-c6e44fe06cb2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset class distribution: Class 0: 900, Class 1: 100\n",
            "Training set class distribution: Class 0: 720, Class 1: 80\n",
            "Testing set class distribution: Class 0: 180, Class 1: 20\n",
            "\n",
            "Training Logistic Regression without class weights...\n",
            "\n",
            "Classification Report (No Class Weights):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95       180\n",
            "           1       0.00      0.00      0.00        20\n",
            "\n",
            "    accuracy                           0.90       200\n",
            "   macro avg       0.45      0.50      0.47       200\n",
            "weighted avg       0.81      0.90      0.85       200\n",
            "\n",
            "Confusion Matrix (No Class Weights):\n",
            "[[180   0]\n",
            " [ 20   0]]\n",
            "\n",
            "Training Logistic Regression with 'balanced' class weights...\n",
            "\n",
            "Classification Report (With Class Weights = 'balanced'):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.57      0.70       180\n",
            "           1       0.12      0.55      0.20        20\n",
            "\n",
            "    accuracy                           0.56       200\n",
            "   macro avg       0.52      0.56      0.45       200\n",
            "weighted avg       0.84      0.56      0.65       200\n",
            "\n",
            "Confusion Matrix (With Class Weights = 'balanced'):\n",
            "[[102  78]\n",
            " [  9  11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "# Solution 14.\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Titanic dataset\n",
        "df = sns.load_dataset('titanic')\n",
        "\n",
        "# Select relevant features and target\n",
        "df = df[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']]\n",
        "\n",
        "# Handle missing values\n",
        "df.dropna(inplace=True)  # Drop rows with missing values\n",
        "\n",
        "# Encode categorical variables\n",
        "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
        "df = pd.get_dummies(df, columns=['embarked'], drop_first=True)\n",
        "\n",
        "# Define features and target\n",
        "X = df.drop('survived', axis=1)\n",
        "y = df['survived']\n",
        "\n",
        "# Feature scaling (optional but helps with convergence)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sc0d_yt8sHeJ",
        "outputId": "82ee1c92-116f-4cc3-9f93-4ec1d8295d08"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.91      0.83        80\n",
            "           1       0.85      0.65      0.74        63\n",
            "\n",
            "    accuracy                           0.80       143\n",
            "   macro avg       0.81      0.78      0.79       143\n",
            "weighted avg       0.81      0.80      0.79       143\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "# Solution 15.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "### 1. Logistic Regression WITHOUT scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "### 2. Logistic Regression WITH Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "### Results\n",
        "print(f\"Accuracy WITHOUT Scaling: {accuracy_no_scaling:.4f}\")\n",
        "print(f\"Accuracy WITH Scaling:    {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI6C2Ew6vAxT",
        "outputId": "bcaae4d7-3c0b-4028-89d4-56280d8b2643"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.9561\n",
            "Accuracy WITH Scaling:    0.9737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "\n",
        "# Solution 16.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the probability of the positive class\n",
        "# The ROC curve and AUC score require probabilities\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC-AUC score\n",
        "# The ROC-AUC score is a measure of how well the model can distinguish between positive and negative classes\n",
        "auc_score = roc_auc_score(y_test, y_prob)\n",
        "print(f\"ROC-AUC Score: {auc_score:.4f}\")\n",
        "\n",
        "# Calculate the ROC curve\n",
        "# The ROC curve is a plot of the True Positive Rate (sensitivity) vs. the False Positive Rate (1 - specificity)\n",
        "# at various threshold settings.\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_score:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Sensitivity)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "M3-8NmRFvTI_",
        "outputId": "9f567ea6-a91b-43df-c64a-b16e3bd4dd4b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9126\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "# Solution 17.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset (using Iris as an example)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model with a custom C value\n",
        "# C=0.5 means moderate regularization (since it's the inverse of regularization strength)\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression model accuracy with C=0.5: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5gyodelvlAv",
        "outputId": "e4c8d324-811d-4770-8821-ff9d6b5d67ea"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model accuracy with C=0.5: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "# Solution 18.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_breast_cancer # Example dataset\n",
        "\n",
        "# 1. Load and prepare data\n",
        "# For demonstration, using the Breast Cancer dataset built into scikit-learn\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train the Logistic Regression model\n",
        "# Using 'liblinear' solver for smaller datasets and L1/L2 regularization\n",
        "model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Identify important features based on coefficients\n",
        "# Coefficients represent the log-odds change for a one-unit increase in the feature\n",
        "# Higher absolute values indicate greater importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Coefficient': model.coef_[0] # For binary classification, there's one set of coefficients\n",
        "})\n",
        "\n",
        "# Sort by absolute coefficient value to see the most impactful features\n",
        "feature_importance['Abs_Coefficient'] = abs(feature_importance['Coefficient'])\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"Feature Importance based on Logistic Regression Coefficients:\")\n",
        "print(feature_importance)\n",
        "\n",
        "# Optional: Evaluate model performance\n",
        "accuracy = model.score(X_test, y_test)\n",
        "print(f\"\\nModel Accuracy on Test Set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRoOaZIzv2sp",
        "outputId": "31a563b9-ca42-496b-83cf-142706f47b59"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance based on Logistic Regression Coefficients:\n",
            "                    Feature  Coefficient  Abs_Coefficient\n",
            "0               mean radius     2.132484         2.132484\n",
            "26          worst concavity    -1.617969         1.617969\n",
            "11            texture error     1.442984         1.442984\n",
            "20             worst radius     1.232150         1.232150\n",
            "25        worst compactness    -1.208985         1.208985\n",
            "28           worst symmetry    -0.742764         0.742764\n",
            "6            mean concavity    -0.651940         0.651940\n",
            "27     worst concave points    -0.615251         0.615251\n",
            "5          mean compactness    -0.415569         0.415569\n",
            "21            worst texture    -0.404581         0.404581\n",
            "7       mean concave points    -0.344456         0.344456\n",
            "12          perimeter error    -0.303857         0.303857\n",
            "24         worst smoothness    -0.262631         0.262631\n",
            "8             mean symmetry    -0.207613         0.207613\n",
            "1              mean texture     0.152772         0.152772\n",
            "2            mean perimeter    -0.145091         0.145091\n",
            "4           mean smoothness    -0.142636         0.142636\n",
            "29  worst fractal dimension    -0.116960         0.116960\n",
            "13               area error    -0.072569         0.072569\n",
            "10             radius error    -0.050034         0.050034\n",
            "16          concavity error    -0.044886         0.044886\n",
            "18           symmetry error    -0.041752         0.041752\n",
            "17     concave points error    -0.037719         0.037719\n",
            "22          worst perimeter    -0.036209         0.036209\n",
            "9    mean fractal dimension    -0.029774         0.029774\n",
            "23               worst area    -0.027087         0.027087\n",
            "14         smoothness error    -0.016159         0.016159\n",
            "19  fractal dimension error     0.005613         0.005613\n",
            "15        compactness error    -0.001907         0.001907\n",
            "3                 mean area    -0.000829         0.000829\n",
            "\n",
            "Model Accuracy on Test Set: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen’s Kappa Score.\n",
        "\n",
        "# Solution 19.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy (optional, but useful for comparison)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression model accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "# Cohen's kappa measures the agreement between two annotators (in this case,\n",
        "# the true labels and the model's predictions). It corrects for the agreement\n",
        "# that would occur by chance.\n",
        "# A score of 1 indicates perfect agreement, 0 indicates agreement equivalent to\n",
        "# random chance, and negative scores indicate agreement worse than random chance.\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geF0zH3NwVnd",
        "outputId": "72431332-a5f1-4ec8-d218-f9f69949c8f9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model accuracy: 0.8300\n",
            "Cohen's Kappa Score: 0.6581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio.\n",
        "\n",
        "# Solution 20.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict the probability of the positive class\n",
        "# The Precision-Recall curve requires probabilities\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision, recall, and thresholds\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "# Calculate the Area Under the Precision-Recall Curve (AUPRC)\n",
        "auprc = auc(recall, precision)\n",
        "print(f\"Area Under the Precision-Recall Curve (AUPRC): {auprc:.4f}\")\n",
        "\n",
        "# Plot the Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR curve (AUPRC = {auprc:.4f})')\n",
        "plt.xlabel('Recall (Sensitivity)')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "OC_iEx8JwmSm",
        "outputId": "0dea7208-1c1e-4704-95df-21f25f61ad27"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under the Precision-Recall Curve (AUPRC): 0.9363\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "# Solution 21.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "# Load the dataset (using Iris as an example)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the list of solvers to compare\n",
        "solvers_to_compare = ['liblinear', 'lbfgs', 'saga']\n",
        "\n",
        "# Dictionary to store accuracies for each solver\n",
        "solver_accuracies = {}\n",
        "\n",
        "print(\"Comparing Logistic Regression Solvers:\")\n",
        "\n",
        "for solver in solvers_to_compare:\n",
        "    try:\n",
        "        print(f\"\\nTraining with solver: {solver}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Create and train the Logistic Regression model with the current solver\n",
        "        # Adjust max_iter if necessary, especially for 'saga' and 'lbfgs' on larger datasets\n",
        "        # 'liblinear' only supports 'l1' and 'l2' penalties.\n",
        "        # 'lbfgs', 'saga' support 'l2' and 'none'.\n",
        "        # 'saga' also supports 'l1' and 'elasticnet'.\n",
        "        # Ensure the penalty is compatible with the solver. We'll stick to default 'l2' for simplicity.\n",
        "        if solver == 'liblinear':\n",
        "            # liblinear is generally good for small datasets and binary classification\n",
        "            model = LogisticRegression(solver=solver, penalty='l2', max_iter=200)\n",
        "        else:\n",
        "            # lbfgs and saga are generally better for larger datasets and multiclass\n",
        "            model = LogisticRegression(solver=solver, penalty='l2', max_iter=1000) # Increased max_iter\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict using the test set\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate and print the accuracy\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        end_time = time.time()\n",
        "        training_time = end_time - start_time\n",
        "\n",
        "        solver_accuracies[solver] = accuracy\n",
        "        print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  Training Time: {training_time:.4f} seconds\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"  Could not train with solver {solver}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  An error occurred with solver {solver}: {e}\")\n",
        "\n",
        "print(\"\\n--- Comparison Summary ---\")\n",
        "for solver, accuracy in solver_accuracies.items():\n",
        "    print(f\"Solver '{solver}': Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "# Optional: Find the best performing solver\n",
        "if solver_accuracies:\n",
        "    best_solver = max(solver_accuracies, key=solver_accuracies.get)\n",
        "    print(f\"\\nBest performing solver based on accuracy: '{best_solver}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HKGl1EqxbGj",
        "outputId": "3448ed66-4cd9-4259-bb24-b6476ce14098"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Logistic Regression Solvers:\n",
            "\n",
            "Training with solver: liblinear\n",
            "  Accuracy: 1.0000\n",
            "  Training Time: 0.0045 seconds\n",
            "\n",
            "Training with solver: lbfgs\n",
            "  Accuracy: 1.0000\n",
            "  Training Time: 0.0174 seconds\n",
            "\n",
            "Training with solver: saga\n",
            "  Accuracy: 1.0000\n",
            "  Training Time: 0.0533 seconds\n",
            "\n",
            "--- Comparison Summary ---\n",
            "Solver 'liblinear': Accuracy = 1.0000\n",
            "Solver 'lbfgs': Accuracy = 1.0000\n",
            "Solver 'saga': Accuracy = 1.0000\n",
            "\n",
            "Best performing solver based on accuracy: 'liblinear'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "# Solution 22.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef, accuracy_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy (optional, but useful for comparison)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression model accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient (MCC)\n",
        "# MCC is a single score that summarizes the confusion matrix.\n",
        "# It takes into account true positives, true negatives, false positives, and false negatives.\n",
        "# A score of +1 represents a perfect prediction, 0 an average random prediction,\n",
        "# and -1 a perfect inverse prediction. MCC is generally considered a balanced\n",
        "# measure that can be used even if the classes are of very different sizes.\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYpygrHuxzs7",
        "outputId": "5d39b671-18bc-421c-d31b-60012aaafb0d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression model accuracy: 0.8300\n",
            "Matthews Correlation Coefficient (MCC): 0.6593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "# Solution 23.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "### 1. Logistic Regression WITHOUT scaling\n",
        "print(\"--- Training without Feature Scaling ---\")\n",
        "model_no_scaling = LogisticRegression(max_iter=200)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "print(f\"Accuracy WITHOUT Scaling: {accuracy_no_scaling:.4f}\")\n",
        "\n",
        "### 2. Logistic Regression WITH Standardization\n",
        "print(\"\\n--- Training with Standardization ---\")\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and test data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train Logistic Regression model on scaled data\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy WITH Scaling:    {accuracy_scaled:.4f}\")\n",
        "\n",
        "### Comparison\n",
        "print(\"\\n--- Comparison ---\")\n",
        "print(f\"Accuracy difference (Scaled - No Scaling): {accuracy_scaled - accuracy_no_scaling:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGzyCA_Rx-Cv",
        "outputId": "a2532eb1-75c0-478c-ad92-7b3e8df14b5d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training without Feature Scaling ---\n",
            "Accuracy WITHOUT Scaling: 0.9561\n",
            "\n",
            "--- Training with Standardization ---\n",
            "Accuracy WITH Scaling:    0.9737\n",
            "\n",
            "--- Comparison ---\n",
            "Accuracy difference (Scaled - No Scaling): 0.0175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "# Solution 24.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset (using Iris as an example)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the data into training and validation sets (adjust split ratio as needed)\n",
        "# We split initially to keep a separate test set if needed, but for finding optimal C\n",
        "# we'll use cross-validation *on the training set*.\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the range of C values to test\n",
        "# C is the inverse of regularization strength. Smaller C means stronger regularization.\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "# Create a Logistic Regression model instance\n",
        "# Use a solver that is generally robust, like 'liblinear' for smaller datasets or 'lbfgs'/'saga'\n",
        "# For GridSearchCV with a range of C, 'liblinear' or 'lbfgs' are common choices.\n",
        "# 'lbfgs' is the default and works well for many cases.\n",
        "logreg = LogisticRegression(max_iter=200) # Increased max_iter for convergence\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "# This will systematically search through the param_grid using cross-validation\n",
        "# cv=5 means 5-fold cross-validation on the training data\n",
        "# scoring='accuracy' is the metric to evaluate the models\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "# GridSearchCV will train the model multiple times with different C values\n",
        "# and different cross-validation folds\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best C value found by GridSearchCV\n",
        "print(\"Best C value found by GridSearchCV:\")\n",
        "print(grid_search.best_params_['C'])\n",
        "\n",
        "# Print the best cross-validation score achieved with the best C\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Optional: Get the best model trained during the grid search\n",
        "# You can then evaluate this best model on the separate validation set (X_val, y_val)\n",
        "# if you kept one.\n",
        "# best_model = grid_search.best_estimator_\n",
        "# validation_accuracy = best_model.score(X_val, y_val)\n",
        "# print(f\"Accuracy of the best model on the validation set: {validation_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d458B0gKyK0W",
        "outputId": "ac9a7779-5ada-4fa7-f969-e8bd0f92c7f7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C value found by GridSearchCV:\n",
            "1\n",
            "Best cross-validation accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ques 25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "# Solution 25.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib  # Import joblib for saving and loading models\n",
        "import os      # Import os for checking file existence\n",
        "\n",
        "# --- 1. Train the Logistic Regression Model ---\n",
        "\n",
        "# Load the dataset (using Iris as an example)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Labels\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model (optional)\n",
        "accuracy_trained = accuracy_score(y_test, model.predict(X_test))\n",
        "print(f\"Accuracy of the freshly trained model: {accuracy_trained:.4f}\")\n",
        "\n",
        "# --- 2. Save the Trained Model using joblib ---\n",
        "\n",
        "# Define the filename for the saved model\n",
        "model_filename = 'logistic_regression_model.joblib'\n",
        "\n",
        "# Save the model to the file\n",
        "joblib.dump(model, model_filename)\n",
        "\n",
        "print(f\"\\nTrained model saved to '{model_filename}'\")\n",
        "\n",
        "# --- 3. Load the Saved Model and Make Predictions ---\n",
        "\n",
        "# Check if the model file exists before loading\n",
        "if os.path.exists(model_filename):\n",
        "    print(f\"\\nLoading model from '{model_filename}'...\")\n",
        "\n",
        "    # Load the model from the file\n",
        "    loaded_model = joblib.load(model_filename)\n",
        "\n",
        "    print(\"Model loaded successfully.\")\n",
        "\n",
        "    # Make predictions using the loaded model\n",
        "    y_pred_loaded = loaded_model.predict(X_test)\n",
        "\n",
        "    # Evaluate the loaded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIT1RHR_zCfO",
        "outputId": "e9884d01-4159-4c4a-de53-cce2971f60a3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the freshly trained model: 1.0000\n",
            "\n",
            "Trained model saved to 'logistic_regression_model.joblib'\n",
            "\n",
            "Loading model from 'logistic_regression_model.joblib'...\n",
            "Model loaded successfully.\n"
          ]
        }
      ]
    }
  ]
}